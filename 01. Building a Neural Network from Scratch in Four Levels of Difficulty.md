---
title: "ðŸŽ“ Building a Neural Network from Scratch in Four Levels of Difficulty"
layout: default
nav_order: 1
parent: "Lecture Notes"
description: "Lecture notes: ðŸŽ“ Building a Neural Network from Scratch in Four Levels of Difficulty"
last_modified_date: 2026-01-11
source_transcript: "001_I Built a Neural Network from Scratch in SCRATCH"
generated_by: "NoteSmith"
---

# ðŸŽ“ Building a Neural Network from Scratch in Four Levels of Difficulty

## ðŸ“‘ Table of Contents
1. ðŸ“– Introduction
2. ðŸ“ Main Content
   - ðŸ¤– Level 1: Building a Neural Network with PyTorch
   - ðŸ“Š Level 2: Building a Neural Network with NumPy
   - ðŸ’» Level 3: Building a Neural Network with C
   - ðŸ§© Level 4: Building a Neural Network in Scratch
3. âš¡ Quick Reference
4. ðŸ“Š Summary Table
5. ðŸŽ¯ Key Takeaways
6. ðŸ’¡ Pro Tips

---

## ðŸ“– Introduction

In this lecture, we explore the process of building a neural network from scratch at four levels of increasing difficulty: **PyTorch**, **NumPy**, **C**, and finally, **Scratch**. The goal is to create a neural network capable of recognizing handwritten digits (zeros and ones) from the EMNIST dataset. This journey not only showcases the power of modern deep learning libraries but also provides a deeper understanding of the underlying mechanics of neural networks.

### Why This Matters
- **Understanding the Basics**: Building a neural network from scratch helps in understanding the core mathematical operations and concepts behind deep learning.
- **Tool Comparison**: By implementing the same model in different tools and languages, we gain insights into their strengths and limitations.
- **Skill Development**: This exercise sharpens programming, debugging, and problem-solving skills across different platforms.

---

## ðŸ“ Main Content

### ðŸ¤– Level 1: Building a Neural Network with PyTorch

#### Overview
PyTorch is one of the most popular deep learning libraries due to its simplicity and flexibility. It abstracts much of the complexity involved in building and training neural networks, making it a great starting point.

#### Steps to Build the Neural Network
1. **Dataset Preparation**:
   - Download the **EMNIST dataset** (a dataset of 28x28 grayscale images of handwritten zeros and ones).
   - Uncompress the files to access **12,665 images**.
   - Split the dataset into **training**, **development**, and **testing sets**.
   - Visualize the images to ensure proper loading.

2. **Define the Neural Network Architecture**:
   - Input layer: 784 neurons (28x28 pixels flattened into a single vector).
   - Hidden layer: 100 neurons.
   - Output layer: 1 neuron (for binary classification: 0 or 1).

3. **Training the Neural Network**:
   - Use PyTorch's `torch.nn.Sequential` to define the architecture.
   - Set up **data loaders** for efficient data feeding.
   - Define the **loss function**:
     > **Definition**: A loss function quantifies how well or poorly the neural network's predictions match the actual labels.  
     > **Type Used**: Binary Cross-Entropy Loss.
   - Choose an **optimizer**:
     > **Definition**: An optimizer adjusts the neural network's parameters to minimize the loss.  
     > **Type Used**: Stochastic Gradient Descent (SGD).
   - Create a training loop:
     - Perform a **forward pass** to calculate predictions.
     - Compute the **loss**.
     - Use **backpropagation** to calculate gradients.
     - Update the parameters using the optimizer.

#### Results
- Achieved **99% accuracy** on the test set after a few seconds of training.
- PyTorch made the process straightforward and efficient.

---

### ðŸ“Š Level 2: Building a Neural Network with NumPy

#### Overview
Using NumPy requires a deeper understanding of the mathematical operations behind neural networks. Unlike PyTorch, NumPy does not provide built-in tools for automatic differentiation or training loops.

#### Key Concepts
- **Neuron**:
  > A neuron takes multiple inputs, multiplies them by weights, adds a bias, and applies an activation function to produce an output.
- **Activation Function**:
  > A mathematical function that introduces non-linearity to the model, enabling it to learn complex patterns.

#### Steps to Build the Neural Network
1. **Forward Pass**:
   - Represent inputs, weights, and biases as matrices.
   - Perform matrix multiplication (`np.dot`) to compute the weighted sum of inputs.
   - Add biases and apply activation functions to introduce non-linearity.

2. **Loss Function**:
   - Measure the difference between the predicted output and the actual label.
   - Use the **Binary Cross-Entropy Loss**.

3. **Backward Pass**:
   - Compute the gradients of the loss with respect to the weights and biases using **partial derivatives** and the **chain rule**.
   - Update the parameters to minimize the loss.

#### Challenges
- Manual implementation of matrix operations and gradient calculations.
- Translating mathematical equations into code.

#### Results
- The implementation required under 200 lines of code.
- Achieved functional results with a deeper understanding of the math behind neural networks.

---

### ðŸ’» Level 3: Building a Neural Network with C

#### Overview
Building a neural network in C is significantly more challenging due to the lack of high-level libraries like NumPy. This requires manual implementation of matrix operations and careful memory management.

#### Key Challenges
1. **Matrix Representation**:
   - In C, n-dimensional matrices are stored as 1D arrays in memory.
   - Used pointer arithmetic to index into memory and simulate matrix operations.

2. **Manual Implementation**:
   - Recreated functions like `np.dot` and activation functions from scratch.
   - Exported images from Python into binary files for loading in C.

3. **Memory Management**:
   - Ensured proper allocation and deallocation of memory to avoid leaks.

#### Steps to Build the Neural Network
- Followed the same process as in NumPy:
  1. Implemented the forward pass.
  2. Calculated the loss.
  3. Computed gradients for the backward pass.
  4. Updated parameters.

#### Results
- Successfully implemented the neural network in C.
- Required significantly more effort and time compared to NumPy and PyTorch.

---

### ðŸ§© Level 4: Building a Neural Network in Scratch

#### Overview
Scratch, a block-based visual programming language, is not designed for building neural networks. This level required creative problem-solving and simplifications to make it work.

#### Key Adjustments
1. **Downsized Images**:
   - Reduced image size from **28x28** to **10x10** to minimize the number of input neurons and parameters.
2. **Imported Pre-trained Weights**:
   - Trained the neural network in NumPy and imported the weights and biases into Scratch as lists for debugging and validation.

#### Steps to Build the Neural Network
1. **Dataset Preparation**:
   - Imported the EMNIST dataset into Scratch as lists.
   - Visualized the images to ensure proper loading.

2. **Forward Pass**:
   - Implemented matrix operations using simple arithmetic blocks.
   - Visualized neuron activations for debugging.

3. **Loss Function**:
   - Created a large Scratch block to calculate the loss.

4. **Backward Pass**:
   - Implemented gradient calculations and parameter updates using Scratch blocks.
   - Reduced the number of hidden neurons to 10 for simplicity.

#### Challenges
- Scratch lacks support for matrices, requiring manual implementation of all operations.
- Debugging was difficult due to Scratch's 1-based indexing.
- Training was initially slow, but enabling **Turbo Mode** significantly improved performance.

#### Results
- Successfully built and trained a functional neural network in Scratch.
- Achieved decent accuracy despite limitations.

---

## âš¡ Quick Reference

### Key Commands and Functions
| Tool    | Key Functions/Commands                                                                 |
|---------|----------------------------------------------------------------------------------------|
| PyTorch | `torch.nn.Sequential`, `torch.optim.SGD`, `torch.nn.BCELoss`                          |
| NumPy   | `np.dot` (matrix multiplication), manual gradient calculations                        |
| C       | Pointer arithmetic for matrix indexing, manual implementation of matrix operations    |
| Scratch | Visual programming blocks for arithmetic, lists for matrix representation             |

---

## ðŸ“Š Summary Table

| Level   | Tool      | Key Features                                                                 | Challenges                          | Results                     |
|---------|-----------|------------------------------------------------------------------------------|-------------------------------------|-----------------------------|
| Level 1 | PyTorch   | High-level API, automatic differentiation, fast training                    | None                                | 99% accuracy in seconds     |
| Level 2 | NumPy     | Manual matrix operations, deeper understanding of math                      | Tedious gradient calculations       | Functional, under 200 lines |
| Level 3 | C         | Pointer arithmetic, manual memory management                                | Complex implementation, time-consuming | Works, slower development   |
| Level 4 | Scratch   | Visual programming, creative problem-solving                                | No matrix support, slow training    | Functional, decent accuracy |

---

## ðŸŽ¯ Key Takeaways

- Neural networks are composed of layers of neurons that process inputs to produce predictions.
- The **loss function** quantifies prediction errors, and the **optimizer** adjusts parameters to minimize the loss.
- PyTorch simplifies neural network implementation with high-level abstractions.
- NumPy provides a good balance between control and abstraction but requires manual implementation of key operations.
- Implementing neural networks in low-level languages like C demands a deep understanding of memory management and matrix operations.
- Scratch, while not designed for such tasks, can still be used creatively to build functional neural networks.

---

## ðŸ’¡ Pro Tips

1. **Start Simple**: Begin with high-level libraries like PyTorch to understand the basics before diving into lower-level implementations.
2. **Visualize Your Network**: Visualizing neuron activations and connections can help with debugging and understanding.
3. **Leverage Pre-trained Models**: Use pre-trained weights and biases for validation when working with unconventional tools like Scratch.
4. **Use Turbo Mode in Scratch**: This can significantly speed up computations in Scratch.
5. **Master the Math**: Understanding concepts like gradients, partial derivatives, and matrix operations is crucial for implementing neural networks from scratch.

--- 

These notes serve as a comprehensive guide to understanding the process of building neural networks at various levels of abstraction. Whether you're a beginner or an advanced learner, this guide provides valuable insights into both the "how" and "why" of neural network implementation.